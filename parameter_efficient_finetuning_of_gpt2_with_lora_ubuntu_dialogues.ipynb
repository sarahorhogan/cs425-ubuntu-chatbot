{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwvR48PAnmXm",
        "outputId": "cbad95a0-fea1-4a26-a6b3-55c2e229b884"
      },
      "outputs": [],
      "source": [
        "!pip install keras-nlp -q\n",
        "!pip install transformers\n",
        "!pip install tensorflow --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POeozaJmnxqu",
        "outputId": "f88ac9c4-2e8c-4f60-d462-1ab790491d4e"
      },
      "outputs": [],
      "source": [
        "import keras_nlp\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import time\n",
        "import pandas as pd\n",
        "from transformers import GPT2Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow import keras\n",
        "\n",
        "policy = keras.mixed_precision.Policy(\"mixed_float16\")\n",
        "keras.mixed_precision.set_global_policy(policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34KCN3HSn0d-"
      },
      "outputs": [],
      "source": [
        "# General hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "NUM_BATCHES = 500\n",
        "EPOCHS = 1  # Can be set to a higher value for better results\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "MAX_GENERATION_LENGTH = 200\n",
        "EPOCHS = 1\n",
        "\n",
        "GPT2_PRESET = \"gpt2_base_en\"\n",
        "\n",
        "# LoRA-specific hyperparameters\n",
        "RANK = 4\n",
        "ALPHA = 32.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZWGY8rToQuX"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, input_text, max_length=200):\n",
        "    start = time.time()\n",
        "\n",
        "    output = model.generate(input_text, max_length=max_length)\n",
        "    print(\"\\nOutput:\")\n",
        "    print(output)\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"Total Time Elapsed: {end - start:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQBxp9Meozbt"
      },
      "outputs": [],
      "source": [
        "def get_optimizer_and_loss():\n",
        "    optimizer = keras.optimizers.AdamW(\n",
        "        learning_rate=5e-5,\n",
        "        weight_decay=0.01,\n",
        "        epsilon=1e-6,\n",
        "        global_clipnorm=1.0,  # Gradient clipping.\n",
        "    )\n",
        "    # Exclude layernorm and bias terms from weight decay.\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"bias\"])\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"gamma\"])\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"beta\"])\n",
        "\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    return optimizer, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m1wSzCvrDRT",
        "outputId": "c0fe84f9-0ebf-4c91-c010-b32301982ec0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c730G7r8r3wH"
      },
      "outputs": [],
      "source": [
        "csv_file_path = \"/content/drive/My Drive/dialogue_pairs.csv\"\n",
        "ubuntu_dialogues = pd.read_csv(csv_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYPsjt2vpAYj"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "class LoraLayer(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        original_layer,\n",
        "        rank=8,\n",
        "        alpha=32,\n",
        "        trainable=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # We want to keep the name of this layer the same as the original\n",
        "        # dense layer.\n",
        "        original_layer_config = original_layer.get_config()\n",
        "        name = original_layer_config[\"name\"]\n",
        "\n",
        "        kwargs.pop(\"name\", None)\n",
        "\n",
        "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
        "\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self._scale = alpha / rank\n",
        "\n",
        "        self._num_heads = original_layer_config[\"output_shape\"][-2]\n",
        "        self._hidden_dim = self._num_heads * original_layer_config[\"output_shape\"][-1]\n",
        "\n",
        "        # Layers.\n",
        "\n",
        "        # Original dense layer.\n",
        "        self.original_layer = original_layer\n",
        "        # No matter whether we are training the model or are in inference mode,\n",
        "        # this layer should be frozen.\n",
        "        self.original_layer.trainable = False\n",
        "\n",
        "        # LoRA dense layers.\n",
        "        self.A = keras.layers.Dense(\n",
        "            units=rank,\n",
        "            use_bias=False,\n",
        "            # Note: the original paper mentions that normal distribution was\n",
        "            # used for initialization. However, the official LoRA implementation\n",
        "            # uses \"Kaiming/He Initialization\".\n",
        "            kernel_initializer=keras.initializers.VarianceScaling(\n",
        "                scale=math.sqrt(5), mode=\"fan_in\", distribution=\"uniform\"\n",
        "            ),\n",
        "            trainable=trainable,\n",
        "            name=f\"lora_A\",\n",
        "        )\n",
        "        # B has the same `equation` and `output_shape` as the original layer.\n",
        "        # `equation = abc,cde->abde`, where `a`: batch size, `b`: sequence\n",
        "        # length, `c`: `hidden_dim`, `d`: `num_heads`,\n",
        "        # `e`: `hidden_dim//num_heads`. The only difference is that in layer `B`,\n",
        "        # `c` represents `rank`.\n",
        "        self.B = keras.layers.EinsumDense(\n",
        "            equation=original_layer_config[\"equation\"],\n",
        "            output_shape=original_layer_config[\"output_shape\"],\n",
        "            kernel_initializer=\"zeros\",\n",
        "            trainable=trainable,\n",
        "            name=f\"lora_B\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        original_output = self.original_layer(inputs)\n",
        "        if self.trainable:\n",
        "            # If we are fine-tuning the model, we will add LoRA layers' output\n",
        "            # to the original layer's output.\n",
        "            lora_output = self.B(self.A(inputs)) * self._scale\n",
        "            return original_output + lora_output\n",
        "\n",
        "        # If we are in inference mode, we \"merge\" the LoRA layers' weights into\n",
        "        # the original layer's weights - more on this in the text generation\n",
        "        # section!\n",
        "        return original_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKvwE23vpRWo"
      },
      "outputs": [],
      "source": [
        "# Load the original model.\n",
        "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=128,\n",
        ")\n",
        "lora_model = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    preprocessor=preprocessor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8SxeQ4sDIEK"
      },
      "outputs": [],
      "source": [
        "# Filter out rows with missing values\n",
        "ubuntu_dialogues = ubuntu_dialogues.dropna(subset=[\"Question\", \"Answer\"])\n",
        "\n",
        "ubuntu_dialogues = ubuntu_dialogues.head(5)\n",
        "\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# Create a TextVectorization layer\n",
        "vectorize_layer = TextVectorization(output_mode=\"int\", max_tokens=20000, output_sequence_length=128)\n",
        "vectorize_layer.adapt(ubuntu_dialogues[\"Question\"].values)\n",
        "\n",
        "# Tokenize and preprocess the data\n",
        "preprocessed_data = vectorize_layer(ubuntu_dialogues[\"Question\"].values)\n",
        "\n",
        "# Create a TensorFlow dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices(preprocessed_data)\n",
        "\n",
        "# Batch and prefetch the dataset\n",
        "BATCH_SIZE = 32\n",
        "dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Tokenize and preprocess the data\n",
        "#preprocessed_data = preprocessor(ubuntu_dialogues[\"Question\"].values)\n",
        "\n",
        "# # Extract the input tensors\n",
        "# input_ids = preprocessed_data[0]\n",
        "# attention_mask = preprocessed_data[1]\n",
        "\n",
        "# # Create a TensorFlow dataset\n",
        "# dataset = tf.data.Dataset.from_tensor_slices((input_ids, attention_mask))\n",
        "\n",
        "# # Batch and shuffle the dataset\n",
        "# dataset = dataset.batch(BATCH_SIZE).shuffle(buffer_size=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i53JZfxTpp0L"
      },
      "outputs": [],
      "source": [
        "for layer_idx in range(lora_model.backbone.num_layers):\n",
        "    # Change query dense layer.\n",
        "    decoder_layer = lora_model.backbone.get_layer(f\"transformer_layer_{layer_idx}\")\n",
        "    self_attention_layer = decoder_layer._self_attention_layer\n",
        "\n",
        "    # Change query dense layer.\n",
        "    self_attention_layer._query_dense = LoraLayer(\n",
        "        self_attention_layer._query_dense,\n",
        "        rank=RANK,\n",
        "        alpha=ALPHA,\n",
        "        trainable=True,\n",
        "    )\n",
        "\n",
        "    # Change value dense layer.\n",
        "    self_attention_layer._value_dense = LoraLayer(\n",
        "        self_attention_layer._value_dense,\n",
        "        rank=RANK,\n",
        "        alpha=ALPHA,\n",
        "        trainable=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "571-CwdRpucR"
      },
      "outputs": [],
      "source": [
        "for layer in lora_model._flatten_layers():\n",
        "    lst_of_sublayers = list(layer._flatten_layers())\n",
        "\n",
        "    if len(lst_of_sublayers) == 1:  # \"leaves of the model\"\n",
        "        if layer.name in [\"lora_A\", \"lora_B\"]:\n",
        "            layer.trainable = True\n",
        "        else:\n",
        "            layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "FAgm5kl6qvFz",
        "outputId": "1690bdbf-19cb-49e7-b92e-d86ba7318b32"
      },
      "outputs": [],
      "source": [
        "lora_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0B7ZtIbq4uH"
      },
      "outputs": [],
      "source": [
        "optimizer, loss = get_optimizer_and_loss()\n",
        "\n",
        "lora_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uWIlCkzrxsCP",
        "outputId": "ea1e597b-308d-4a83-aa03-5f48523104a3"
      },
      "outputs": [],
      "source": [
        "lora_model.fit(\n",
        "    ubuntu_dialogues[\"Question\"].values,\n",
        "    epochs=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXS-JpgRx-Nz"
      },
      "outputs": [],
      "source": [
        "for layer_idx in range(lora_model.backbone.num_layers):\n",
        "    self_attention_layer = lora_model.backbone.get_layer(\n",
        "        f\"transformer_layer_{layer_idx}\"\n",
        "    )._self_attention_layer\n",
        "\n",
        "    # Merge query dense layer.\n",
        "    query_lora_layer = self_attention_layer._query_dense\n",
        "\n",
        "    A_weights = query_lora_layer.A.kernel  # (768, 1) (a, b)\n",
        "    B_weights = query_lora_layer.B.kernel  # (1, 12, 64) (b, c, d)\n",
        "    increment_weights = tf.einsum(\"ab,bcd->acd\", A_weights, B_weights) * (ALPHA / RANK)\n",
        "    query_lora_layer.original_layer.kernel.assign_add(increment_weights)\n",
        "\n",
        "    # Merge value dense layer.\n",
        "    value_lora_layer = self_attention_layer._value_dense\n",
        "\n",
        "    A_weights = value_lora_layer.A.kernel  # (768, 1) (a, b)\n",
        "    B_weights = value_lora_layer.B.kernel  # (1, 12, 64) (b, c, d)\n",
        "    increment_weights = tf.einsum(\"ab,bcd->acd\", A_weights, B_weights) * (ALPHA / RANK)\n",
        "    value_lora_layer.original_layer.kernel.assign_add(increment_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjD8aVTi115G"
      },
      "outputs": [],
      "source": [
        "lora_model.save('lora_fine_tuned_model')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
