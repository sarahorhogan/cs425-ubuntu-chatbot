{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzYkeZdmHRd8"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf\n",
        "!pip install sentence_transformers\n",
        "!pip install pinecone-client\n",
        "!pip install transformers\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "521SAT1nHWue"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ_wfLS2hkTp"
      },
      "outputs": [],
      "source": [
        "import pinecone\n",
        "#Connect to the vector database \n",
        "pinecone.init(api_key=\"c72b02c0-62fa-4ee2-aa36-6c2384fed7e1\", environment=\"gcp-starter\")\n",
        "pinecone.list_indexes()\n",
        "index = pinecone.Index(\"ubuntu-ir-jina\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-8p27CwLGvH2"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "#Load documentation dataset \n",
        "loader = PyPDFLoader(\"/content/drive/My Drive/ubuntu-server-guide-2023-10-15.pdf\")\n",
        "pages = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS_j1_ZsHfbM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "from numpy.linalg import norm\n",
        "#Load pre-trained model used to create word embeddings \n",
        "cos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n",
        "model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True) # trust_remote_code is needed to use the encode method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2mUgqTc3EjPz"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "#Chunk the documentation dataset according to the specifications below. \n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 8000,\n",
        "    chunk_overlap  = 1000,\n",
        "    length_function = len,\n",
        "    add_start_index = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24C7sDGLH0Ic"
      },
      "outputs": [],
      "source": [
        "#Generate word embeddings of each chunk using the pre-trained model and inserting it into the vector database \n",
        "counter = 0\n",
        "for i in range(len(pages)):\n",
        "    text_file = open(\"page_\"+ str(i) + \".txt\", \"w\")\n",
        "    n = text_file.write(pages[i].page_content)\n",
        "    text_file.close()\n",
        "    with open(\"page_\"+ str(i) + \".txt\") as f:\n",
        "      info = f.read()\n",
        "    texts = text_splitter.create_documents([info])\n",
        "    for j in texts:\n",
        "      embedding = model.encode([j.page_content])\n",
        "      index.upsert([(str(counter),embedding[0].tolist(),{\"text\":j.page_content})])\n",
        "      print(f'Embedding count: {counter}')\n",
        "      counter += 1\n",
        "print(f'Total number of embeddings: {counter}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdUilFVbr-1w"
      },
      "outputs": [],
      "source": [
        "# Query the vector database to retrieve the top k similar embeddings \n",
        "query = \"How do i ssh into an external server?\"\n",
        "\n",
        "embedding = model.encode([query]).tolist()\n",
        "\n",
        "index.query(\n",
        "  vector=embedding,\n",
        "  top_k=15,\n",
        "  include_metadata=True\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
