{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9aA_PTWtFUD"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY-8h38AurIs"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95Y1fsIYkUNs"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYf69CZ6tNsl",
        "outputId": "8585bc93-a756-416d-d4b6-c9c7401b640f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOaL99SDtTPu"
      },
      "outputs": [],
      "source": [
        "# Load a pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"  # Can choose other variations like \"gpt2-medium\" or \"gpt2-large\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81mgwOmJtju-",
        "outputId": "09b8928b-ddb0-492a-86ff-66371d4ff5bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Format dataset as a text file with each question-answer pair on a separate line.\n",
        "dataset = TextDataset( #Might need to convert csv file into txt file before doing this\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"/content/drive/My Drive/dialogue_pairs.csv\",  # Replace with the path to your dataset file\n",
        "    block_size=128,  # Adjust this according to your dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0f2Y3qnt8IZ"
      },
      "outputs": [],
      "source": [
        "# Prepare data for training\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNZGWGNwul0o"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned\",  # Output directory for the fine-tuned model\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=1000,\n",
        "    num_train_epochs=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=1000,\n",
        "    logging_steps=100,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Orp_xDfvPka"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "-G0MWMVbvQru",
        "outputId": "9e2c82cc-be0d-4f39-b83a-d98848762ff3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='825' max='30548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  825/30548 1:59:47 < 72:06:16, 0.11 it/s, Epoch 0.03/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Fine-tune the model\n",
        "for epoch in range(training_args.num_train_epochs):\n",
        "    for step, batch in enumerate(trainer.get_train_dataloader()):\n",
        "        trainer.train()\n",
        "\n",
        "        # Print current timestep and epoch\n",
        "        if step % training_args.logging_steps == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{training_args.num_train_epochs}] - Step [{step}]\")\n",
        "\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}