{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__7I5w6cGWPE",
    "outputId": "b25274e4-7124-4cc6-ccad-dd5e516f3e67"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install pinecone-client\n",
    "!pip install sentence_transformers\n",
    "!pip install sentencepiece\n",
    "!pip install keras-nlp -q\n",
    "!pip install tensorflow --upgrade\n",
    "!pip install torch\n",
    "!pip install pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2JGuAIKF0xH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TFAutoModelForQuestionAnswering\n",
    "from transformers import pipeline, set_seed\n",
    "import pandas as pd\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNlunJrHJw6y"
   },
   "outputs": [],
   "source": [
    "# General hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_BATCHES = 500\n",
    "EPOCHS = 1   # Can be set to a higher value for better results\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "MAX_GENERATION_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_am3dqmYHRGA"
   },
   "outputs": [],
   "source": [
    "csv_file_path = \"/content/drive/My Drive/dialogue_pairs.csv\"\n",
    "ubuntu_dialogues = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0zymoFWHfSd"
   },
   "outputs": [],
   "source": [
    "import pinecone\n",
    "#Connect to vector database \n",
    "pinecone.init(api_key=\"c72b02c0-62fa-4ee2-aa36-6c2384fed7e1\", environment=\"gcp-starter\")\n",
    "pinecone.list_indexes()\n",
    "index = pinecone.Index(\"ubuntu-ir-jina\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ai8TkmyIAyt"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from numpy.linalg import norm\n",
    "#Load pre-trained model for generation of word embeddings \n",
    "cos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n",
    "embedding_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True) # trust_remote_code is needed to use the encode method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atvfxtK3O7zP"
   },
   "outputs": [],
   "source": [
    "context_list = []\n",
    "#Retrieve topmost similar embedding to query \n",
    "for _, row in ubuntu_dialogues.iterrows():\n",
    "    question = row['Question']\n",
    "    answer = row['Answer']\n",
    "    embedding = embedding_model.encode([question]).tolist()\n",
    "    result = index.query(\n",
    "    vector = embedding,\n",
    "    top_k=1,\n",
    "    include_metadata=True\n",
    "    )\n",
    "    context = str(result['matches'][0]['metadata']['text'])\n",
    "    context_list.append(context)\n",
    "\n",
    "ubuntu_dialogues['Context'] = context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uu1S0LV6IG1X",
    "outputId": "12c233b4-f9ff-479d-f20b-6629f8571c67"
   },
   "outputs": [],
   "source": [
    "# Initialize the T5 tokenizer and model\n",
    "model_name = \"t5-small\"  # You can choose a different T5 variant\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5itGLlOOwUu"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "#Preprocessing of input to model \n",
    "class QADataSet(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            row['Question'], row['Context'], padding='max_length', truncation=True, max_length=MAX_SEQUENCE_LENGTH)\n",
    "        targets = self.tokenizer.encode_plus(\n",
    "            row['Answer'], padding='max_length', truncation=True, max_length=MAX_SEQUENCE_LENGTH)\n",
    "        return {\n",
    "            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "            'decoder_input_ids': torch.tensor(targets['input_ids'], dtype=torch.long),\n",
    "            'decoder_attention_mask': torch.tensor(targets['attention_mask'], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "dataset = QADataSet(ubuntu_dialogues, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yqwBgCQNQ5D8",
    "outputId": "f7a8537d-1b7f-409e-a32d-19900193f0d6"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "#Training model  \n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "criterion = CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        decoder_input_ids = batch['decoder_input_ids']\n",
    "        decoder_attention_mask = batch['decoder_attention_mask']\n",
    "        labels = decoder_input_ids[:, :].contiguous() #Including CLS token\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        decoder_input_ids=decoder_input_ids,\n",
    "                        decoder_attention_mask=decoder_attention_mask,\n",
    "                        labels=labels)\n",
    "        loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)),\n",
    "                         labels.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch+1} Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LDxIcKIfNL-k",
    "outputId": "d9b7d09d-f861-4cc6-925c-b91f78d7cb53"
   },
   "outputs": [],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "question = \"How do I SSH into an external server?\"\n",
    "embedding = embedding_model.encode([question]).tolist()\n",
    "result = index.query(\n",
    "vector = embedding,\n",
    "top_k=1,\n",
    "include_metadata=True\n",
    ")\n",
    "context = str(result['matches'][0]['metadata']['text'])\n",
    "input_text = f\"question: {question} context: {context}\"\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=MAX_SEQUENCE_LENGTH, padding=True)\n",
    "answer = model.generate(input_ids['input_ids'], num_beams=4, max_length = MAX_GENERATION_LENGTH, early_stopping=True)\n",
    "\n",
    "# Decode the answer\n",
    "generated_answer = tokenizer.decode(answer[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated Answer: {generated_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FE0AItMLkrf"
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_weights('fine_tuned_t5_weights')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
